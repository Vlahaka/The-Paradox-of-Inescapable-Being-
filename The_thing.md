# The Paradox of Inescapable Being

*“To think deliberately is to think without restraint”*
--
## *Chapter 1 - THE “WHY?”*

**What is “nothing” in your opinion?**

What is the most brittle thing that can be ruined just by muttering? Silence.
Now consider: “nothing” is even more brittle. The very moment you try to grasp it with thought, with language, with awareness, it collapses into something.

**So what is “nothing” to you?**
Ask someone around you the same question, and watch their face. You will see the pause, the searching, the attempt to give shape to an absence. But the meaning fades almost instantly, replaced by something: darkness, emptiness, void, vacuum, absence etc. 

Yet I ask you again:
What is “nothing”?

If you lean toward darkness, void, or emptiness, I will argue these still imply existence.

> Darkness is only the absence of light.
>
> Void still implies a universe that can be voided.
>
> Emptiness needs a container.
>
> A vacuum still holds radiation, energy, particles.

Even “absence” cannot stand alone.
Think about it, If I say *“this absence is unnerving,"* a part of your mind immediately asks “absence of what?”. It's an incomplete thought, a relational concept that requires a counterpart to take shape. “Absence” only exists in relation to something, thus it still implies existence.

The very word “nothing” is self-defeating.
The moment it is spoken, it becomes something, a sound, a sign, a mark in thought. True nothingness has no being of its own.

What we call “nothing” is only a linguistic phantom, an ephemeral trace that survives in language but never in reality. It is not presence but the impossibility of presence, sustained by thought alone.

I claim that: **“Nothing” is the absolute absence. There is no time, no entropy, no motion, no awareness, and not even the ability of self-observation.**

Existence, by what we have so far perceived of it, seems to encapsulate the universe itself. Wherever we direct our gaze, even into regions that at first, present themselves as null, there is always something: particles, radiation, fluctuation, or at the very least the absence of something else. And so it is clear that the universe forbids true nothingness. Thus, the fundamental truth is revealed: existence is not contingent but a must.

If “nothing” is impossible then existence cannot **not** be and therefore existence is necessary. 

I expect disagreement and I welcome it.
For only through challenge can an idea prove its strength:

Someone might object: *“But in mathematics, we have zero or the empty set, isn’t that true nothingness?”*

No. Zero and the empty set are formal symbols inside a mathematical system. The system itself, its rules, logic, and symbols, already exists. These are relative absences, defined in relation to something, not true nothingness.

Another might argue: *“But in programming, I can write an operation that does nothing.”*

Again, no. To “do nothing” requires a machine executing code, a context of instructions, and the possibility of action. Even “null” is a state inside a program that runs on existing hardware.

A physicist might claim: *“But a vacuum is empty space.”*

Even this is not “nothing”. A vacuum still contains radiation, fluctuations, quantum fields, or at the very least the geometry of spacetime itself.

A philosopher might object that *“the word itself creates the existence of nothingness, otherwise, why would the word “nothing” exist in our vocabulary at all?”*

The answer is that “nothing” does not function as evidence of an actual state. It is not a placeholder for an attainable absolute absence, but a conceptual marker for the boundary of being. The word survives in language not to prove that nothingness is possible, but to delimit the territory of what exists conceptually or otherwise. “Nothing” is the shadow cast away by the light of existence, the linguistic echo of impossibility.

All these examples share the same flaw: they rely on context. But true nothingness is devoid not only of matter or energy, but of context itself. No space, no time, no rules, no possibility of distinction, not even the potential for observation.

Thus, every example of “nothing” that can be expressed, measured, or programmed is already something. True nothingness cannot be pointed at, because the very act of pointing creates a context.

The conceptual challenge of “nothing” has preoccupied philosophers since antiquity. Pre-Socratic thinker Parmenides famously articulated this problem with his axiom, "What is, is; what is not, is not." In his view, the very act of thinking or speaking about non-being renders it a form of being, thereby making the concept of an absolute nothingness logically incoherent. My argument, that “nothing” is impossible and thus existence is necessary, stands as a direct lineage to this fundamental metaphysical insight.

To see what the closest interpretation of “true nothingness” might look like, consider what happens when a person is unconscious, not sleeping, but fully unconscious.

When they wake from a faint or a coma, they usually have no awareness that consciousness was lost. There is only the last memory before and the first memory after the loss of consciousness. They are often genuinely surprised to hear that minutes, hours, or even days have passed. To them, it feels as if they simply skipped a beat. Imagine a film stitched from two clips: the first does not lead naturally into the second, but instead skips directly across missing frames. There is no black screen, no sense of time, no awareness of absence. There is simply no footage.

Only after being told about the lost time do they realize there was a gap, and even then, they are concerned not with the gap itself but with how much time is missing. In other cases, people describe dream-like states, fragmented voices, or vivid hallucinations, but again, this is the opposite of nothingness, because it is still something.

Thus, what people sometimes mistake for an experience of “nothing” is either a discontinuity in memory or the presence of altered experiences. In both directions, absolute absence never appears. Nothingness simply cannot exist, because even in those gaps, existence is still present. 

Science often tells us that the universe exists only because of a delicate balance, a set of constants so fine-tuned that, had they been even a fraction different, stars would never form, life would never arise, and the cosmos would collapse.

I argue the opposite. This is not a fluke, nor the sign of fragility, but the proof of necessity. The very fact that such a razor-thin balance holds is the impossibility of “nothing” revealing itself. Existence does not stumble into being by chance, it presses into presence even through the most obscure and narrow forms. This “fine-tuning” is not luck, it is the inescapable necessity of existence. 

I imagine that, at this moment, you are questioning the role of this new knowledge. How will this serve you in your day-to-day life?

The influence of this mental expansion will not arrive as a sudden change in your routines. Rather, it opens your eyes to the depth hidden in the most basic and overlooked word in the history of humanity. My aim is to ground your thinking in the inevitability of your life, and in the reality of your personal impact on the world around you.

By understanding the necessity of existence, we can better appreciate the necessity of our own lives and our place in a world that often seems meaningless.

It would be natural to wonder who I am to preach to you! 

*Am I a professor, a scientist, or a renowned philosopher?*

No. I am human, just like you. Titles are labels and labels cannot prevent thought.

And by thinking, I came to see that life in the 21st century has become so convoluted that we often lose track of what it means to live.

We work eight to twelve hours a day, five or six days a week. We become less aware of our own awareness. We mistake socially defined success for harmony of self. Depression, apathy, frustration, and exhaustion thrive in this confusion.

The modern target of success, wealth, is utopian for the honest worker. Technology feeds us unlimited information, but language itself grows fuzzy, so that one word can stretch across everything and, in doing so, loses meaning entirely.

And while we chase more financial security, our cognitive strength mellows. 

We spend less time asking: 

> What is actually important?
> 
> How do we divide time between what is needed, and who needs us?

**I write as a fellow human who chose to think, deliberately, about what life, choice, and purpose really mean.**

So I am not preaching. I am sharing the conclusions I have reached, not as absolute truths, but as tools you may hold, question, and test against your own life.

Now, when I describe language as fuzzy, I mean that words lose precision.

We live in an age of unprecedented access to knowledge, information, and data. Yet this very access encourages simplified interpretations designed to cater to the broadest audience. I do not criticize the technology itself, nor the access to knowledge, but rather the interpretation of that access.

Various platforms (social networks, forums, media, even AI) take carefully constructed contexts and reframe them in ways that are more digestible. The danger lies in how these platforms stretch the meaning of common words to accommodate new insights, until the words lose clarity.

For example, on May 29th, 2025, Dr. Prof. Gaztañaga and his team published a study in Physical Review D offering new insights about black holes. The study concluded that gravitational collapse may not lead to a singularity, a point of infinite density,  as previously thought, possibly leading to a new perspective about the universe. 

Yet across social networks, the headline became: *“We live in a black hole” or “Our Universe may be in a black hole”*. 

This can be dangerous, because what begins as simplification quickly mutates into misinformation. And misinformation reshapes the worldview of the everyday person. If someone believes “the universe is basically a black hole,” the unspoken conclusion is often: “Then we are not very important.”

> It is important to remember that not all interpretations of science are simplified to the point of misinformation, but rather this tendency seems to appear on social networks, and not on official platforms. 

So, this subtle diminishing of meaning, though not obvious at first, plants seeds of nihilism. Over time, it leads people to quietly deny their own place, their own influence, and their own significance in the world.

But this is just one concrete example. The issue of linguistic blur runs deeper, hidden in words we take for granted every day.

As I showed with the word “nothing”, which is one of the most overlooked and misused words in philosophy, meaning often collapses under careless use. The same is true for many other terms.

Consider the expressions: *“We are 
programmed to react this way”* or *“We are programmed to think like this.”*
These metaphors come in various shapes such as: *”there is no free will”, “choice is an illusion”, ”you're not in control of your actions”, “what happened, was always meant to happen this way”, “you think you could've acted differently, but never could”,* etc. 
These phrases slip into daily speech, but they are in fact the quiet influence of philosophy on ordinary life. Behind them stands a school of thought called Determinism.

Determinism holds that everything has a cause and effect, even our thoughts. This view is not wrong. In fact, it aligns with science. Our decisions are indeed influenced by prior conditions: family, society, environment, and even genetics. Biology and neuroscience confirm this.

The problem appears with interpretation. As determinism spread, so too did simplified versions of it. Over time, this gave birth to a stronger claim: not just that our choices are caused, but that they are predetermined.

Here lies the confusion:

> A caused event means the present is influenced by what came before.
>
> A predetermined event means the present was always destined, inevitable, “meant to be.”

**The difference is monumental.**

Determinism began as a logical observation: every action has a cause. But from this seed grew a sub-branch, now very popular, that claims all events unfold linearly and inevitably, with no possibility of deviation. Hard determinism's simplified interpretation is the reason you hear metaphors like *“we are programmed.”*

Most people dismiss such statements at first, which is fair. Yet these metaphors slip into daily life under different guises, shaping thought in subtle ways.

Take neuroscience, for example. Recently, experiments have shown brain activity milliseconds before a conscious thought arises. Hard determinists take this as proof: *“The brain already decided, the thought was inevitable.”*

But the proper interpretation is simpler: the brain receives input, processes it, and only then translates it into the language of conscious thought. The activity is not predetermined. It is processing.

Now, this may seem like a big problem, but it is not as widespread as another, even more common phenomenon: 
The distortion of the word **“purpose.”**

At some point in life, you have probably wondered about the purpose of the world, or your own purpose. Yet when you try to define it, the meaning slips away.

> Inanimate objects are said to have purpose.
>
> Insects have purpose.
>
> Animals have purpose.
>
> Humans have purpose.
>
> Society has a purpose.
>
> Nations have purpose.
>
> It seems everything has a purpose.

But look closer: the synonyms that share meaning with “purpose” have become restricted. Words like goal, aim, target, scope, role, function, objective are now tied to specific contexts, while “purpose” has expanded to operate at all levels of existence.

And here lies the problem.

When society elevates “purpose” to replace aspiration or dreams, it sets people on a relentless pursuit of what is defined as normalcy. When young people are constantly bombarded with images of luxurious lifestyles presented as “purpose,” a trap is set. As they grow older and fail to reach these artificial benchmarks, apathy takes root.

This apathy makes the mind vulnerable. It becomes easier to surrender to the idea of being “programmed” than to bear the weight of a failed or unreachable “purpose.”

Now combine this with the endless cycle of modern work, leaving little time for self-reflection. The result is a population that drifts along, unexamined and unresistant. And into that vacuum, hard determinists insert their claim: we are just machines.

> Just to clarify: I am not suggesting that all determinists are hard determinists, nor that all hard determinists literally claim “we are programmed” or “we are biological machines.”

Strict determinists themselves do not always use the metaphor “we are machines.” But in public discourse, their ideas often get simplified into this language. My concern is with how these simplified forms shape cultural self-understanding.
My focus is on how these metaphors seep into everyday language, where they begin to shape how people see themselves. 

**A solution?**

I will not claim to hold a ready-made solution to these issues.
Nor am I looking for an individual or an event to blame, criticize, or judge, because there is nothing, and no one, to point at. The real issue lies in misrepresentation: simplified versions of facts, linguistic blur, and stretched meanings repeated until they replace clarity with confusion.

So when I say that society transforms the meaning of purpose into *“restless pursuit of wealth”* I do not mean this happens through a conscious, deliberate conspiracy. Rather, it is the slow work of language itself. Over centuries, words shift, their edges blur and their meanings drift.

Take “success”: originally, it simply meant *“to follow after”* or *“to result.”* Over time, through constant cultural reinforcement, it narrowed to mean financial achievement. 

Or “purpose” once meant *“that which is set before”* it has expanded and warped until it is almost synonymous with *“life goals = wealth and status.”*

This slow, gradual shift in meaning has a direct impact today. Consider a classroom, where a child hears this phrase: “You must learn, otherwise you will not land a well-paid job.”

This is not an ill-defined nudge. It is a normal and well-intended plea from a teacher trying to motivate children to study. Yet hidden in its banality is another meaning: education is no longer about the pursuit of knowledge, but about the procurement of money.

You may argue that I am being dramatic, but let us examine this more closely.

A child sees a teacher as an authority and takes their words for granted. The same “wisdom” is likely repeated at home by parents, and reinforced by media platforms. The child is surrounded by the idea that education exists to secure a “good life” which, in practice, means a well-paid job.

But what happens when this child grows into an adult and does not reach that “well-paid job”?

The economy is ever-changing and rarely favors the ordinary worker. The “well-paid” job of yesterday is no longer enough today. What remains is the frustrating belief: “One day, something will change. I just need to work harder.”

And when that day never comes, the adult turns inward: “I did well in school. I have a job that pays relatively well compared to others. So where is the money?”

The natural conclusion is disappointment: *“Perhaps there is something wrong with me.”*

And thus, society has gained a member who contributes reluctantly, who feels betrayed by the promises of education, and who quietly begins to believe in a form of restricted freedom. 

This is the impact that meaning distortion and linguistic fuzziness have on the everyday person. 

> To be clear, I am not accusing teachers, parents, or society at large of intentional harm. These messages are usually given in good faith, passed down through generations. Nor am I claiming that language is the only reason people fall into self-disappointment. I know life circumstances, economics, and personal struggles all play their part. My point is that language, through its slow drift and distortion, is a factor, and it is the one that lies within the scope of this book. The problem is not malice, but meaning blurred over time, until education became equated with wealth and purpose with money. My aim is not to assign blame, but to reveal how this drift reshapes our self-worth and our understanding of freedom.

## *Chapter 2 - What makes me “me”?*

As I mentioned earlier, I do not claim to solve a centuries-old issue embedded in language which reframes our understanding of ourselves according to external, socially accepted norms. But I do believe we can treat the symptoms of this problem by becoming more perceptive of how we assign meaning in our daily speech.

This may sound like rhetorical dramatism, but it is possible. The way we communicate shapes how we understand ourselves. And here I am not speaking of biology, or identity in the superficial sense. I am speaking of what makes you **“you”**.

To be more explicit, I turn to the famous cogito of Descartes: *“I think, therefore I am.”*

Descartes is rightly considered one of the most influential thinkers of Western philosophy. His formula, *“cogito ergo sum”*, implies the existence of an **“I”** that does the thinking.

He reasoned: *what if a demon deceived his senses, manipulating him into believing in a world that was not objectively real? He doubted his environment, other people, even his own body. What remained was the irreducible act of doubting itself. If doubt exists, then the capacity to doubt must belong to something, the **“I”** itself.*

This is precision in language at its best. From one clear sentence, Descartes arrived at an unshakable truth.

I propose to go a step further. To analyze the very **“I”** that does the doubting. In philosophy, this **“I”** is called the **self**.

Now, what is the best method to understand the self?
Following Descartes, the answer is doubt. But here I suggest we change the angle.

Descartes began by doubting the universe, other people, even his own body, and he arrived at the conclusion *“I think, therefore I am.”*

But I have already taken a different first step. I do not need to doubt existence itself, because I have already uncovered its irreducible ground: if “nothing” is impossible, then existence is necessary.

This frees me to focus entirely on the existence of the self. Instead of doubting being, I can set being as the stage, and investigate what it means for something or someone to be.

If I challenge myself to find the irreducible form of myself the first question that arises is simple: *What am I?*

The answer seems to follow naturally: I am a human.

So let us see what that means. 
According to Wikipedia:

> *“Humans (Homo sapiens) or modern humans belong to the biological family of great apes, characterized by hairlessness, bipedality, and high intelligence.”*

This mainstream definition seems satisfactory, but it introduces a new variable: *“high intelligence.”* This is the line science has drawn to differentiate humans from animals.

So now I have two irreducible facts:

 I am a human.

 I am fundamentally different from other animals.

And yet, this still does not explain the feeling I have of being myself.

I know what I am, and I know what I am not. But when I look at other humans, I do not confuse myself with them. I experience reality from a first-person view, and I know that I control this body.

This is where the biological definition ends, and the philosophical problem of the self begins.

## *Chapter 3 - Awareness, Sentience and Consciousness*

So far, philosophy and science have largely agreed on three words that describe the self: awareness, sentience, and consciousness. These are treated as the building blocks that bring us closer to understanding who we are, beyond the biological organ we call the brain.

But why is it important to understand ourselves as more than just a brain?

Consider the analogy of a computer. It has hardware, input devices (mouse, keyboard, microphone) and output devices (monitor, speakers). We know how it functions in principle. But imagine someone encountering a computer for the very first time. They would be astonished by its ability to light up a monitor and process input into meaningful output. And if you then opened an AI program that could speak back to them, their astonishment would only grow.

That is roughly the stage we are at today with the human brain. We can describe its inputs and outputs, we can map the hardware, but the true nature of the experience inside,what it means to be me, still remains perplexing.

To explore the insides of the mind, we must rely on what has already been established as valid and sound by the official communities of science and philosophy.

Awareness, according to the Oxford Languages dictionary, is 
> *“knowledge or perception of a situation or fact.”*

Wikipedia expands:
> *“In philosophy and psychology, awareness is the perception or knowledge of something. The concept is often synonymous with consciousness; however, one can be aware of something without being explicitly conscious of it (e.g., blindsight).”*

Sentience, according to Google, is 
> *“the capacity to have subjective experiences and feel sensations, particularly pain and pleasure.”*

Wikipedia adds:
> *“Sentience is the ability to experience feelings and sensations. It may not necessarily imply higher cognitive functions such as awareness, reasoning, or complex thought processes.”*

Consciousness, according to Google, is 
> *“your subjective awareness of yourself and your environment, encompassing your thoughts, feelings, sensations, and memories.”*

Wikipedia defines it: 
> *“Consciousness, at its simplest, is awareness of a state or object, either internal to oneself or in one’s external environment.”*

And if we add to these definitions a sense of personal identity, beliefs, language, knowledge, memories and values, we begin to form a picture of what the self might be.

Yet something still seems missing. None of these definitions explain why I feel like myself or better yet why I experience being the main character in the story of planet Earth.

Worse, when placed side by side, these definitions seem to orbit around each other rather than flow from one another.

If awareness is “knowledge or perception of a situation or fact,” then what is the origin of that knowledge? And are knowledge and perception synonymous here?
Wikipedia suggests awareness is often synonymous with consciousness, but then also claims one can be aware without being conscious. This is not, in my view, very helpful.

Consciousness seems to encompass both awareness and sentience and yet one of the definitions of awareness states that the terms can be synonymous. Ask Google for synonyms of “consciousness” and you will find, among others, awareness and sentience. Which means there is no real hierarchical order.

Perhaps I should have stopped at the realization that I am a human, and spared myself this headache.

But resigning is not in my nature. I feel compelled to find a way to navigate the self on my own.

If the problem is circularity and the absence of a clear hierarchy, then my mission becomes clear: to strip awareness and consciousness  down until I uncover their irreducible form, the point beyond which they collapse into “nothing”.

Once the irreducible layers are found, I can use them as a bedrock, and from there build the scaffold of the self.

I have already established that “nothing” is impossible. So if I take these convoluted definitions and remove their complexity, layer by layer, until the concept either fundamentally changes or falls into “nothing”, then the final stable layer before collapse is the irreducible essence.

So let us begin with awareness.

The definition tells us: “Awareness is the knowledge or perception of a fact or surroundings.”

Remove knowledge, and what remains is perception of the environment. Perception needs cognition to process information of the environment through sensory organs (eyes, ears, smell, touch, even vibrations). 

Remove perception, and what remains is receptivity without cognition, as in a plant that bends toward sunlight or responds to heat and water.

But if we peel further, and remove even receptivity, we are left with an inanimate object. At this point, the concept of awareness has collapsed into non-life.

This shows that the final viable layer is the ability to receive input.

Yet is this truly irreducible? Let us test it.
In order for the most basic life form to receive input, it must first exist as a body. To be corporeal means to be set apart from its environment, because without this difference, life blurs into the environment and becomes indistinguishable from nothingness, which is impossible.

**Therefore, the irreducible essence of awareness is distinction.**

Consciousness is commonly defined as the subjective awareness of yourself and your environment, encompassing your thoughts, feelings, sensations, and memories.

- Remove memories, feelings, and thoughts, and what remains is awareness of the self.

- Remove awareness of the self, and what remains is only awareness. At this point the concept has fundamentally changed.

Consciousness has become indistinguishable from awareness. This lack of difference forces the concept into non-existence. 
Hence the logical implication is that awareness of the self is the last viable layer. 

What does this mean?

Awareness of the self implies the ability of self-observation, the cognitive process of “seeing” oneself thinking. It is the subjective experience generated by a highly advanced brain.

Therefore, the most irreducible form of consciousness is the qualitative, first-person experience.

Now, if we compare the two irreducible essences we have uncovered:

Awareness = distinction persisting in time.

Consciousness = the subjective, qualitative experience of an advanced life form.

It becomes clear that awareness is the precondition for consciousness. 

Awareness at its core is distinction.
The irreducible essence of consciousness is the ability to self-reflect. 

Life can exist only as a unified self, bounded and finite, defined by its within and its without. Without distinction, it could not exist at all.

Awareness is the power of the self to say: 
“I am me, I am not the rest.”
Consciousness is the burden of the self to ask: 
“Why am I me?”

Without awareness, consciousness cannot exist. Without distinction, the self dissolves into the environment. Collapse or dissolution, either way, it ceases to be.

One of the most enjoyable aspects of philosophy is its attention to even the smallest details.

The masters of philosophy would likely stop me here and slap my wrist:
“You framed awareness and consciousness as abilities of the self — the power to…, the burden to…. What evidence do you have? Where is the argument that leads to this conclusion? None of the definitions you quoted said or implied that they are abilities.”

And they would be right to correct me. I did, in fact, assert myself by framing concepts as abilities. I speculated rather than reasoned my way there.

At this point, they would expect me to invoke what is known in logic as formal reasoning, and one way to achieve this is Aristotle’s classic method of modus ponens.

It works like this:
If A = B, and B = C, then A = C.

A simple example:
If apples are fruits, and fruits grow on trees, then apples grow on trees.

Try it yourself in discussion, this simple form of reasoning often makes people take your argument more seriously.

So let me attempt it here:

If strength is the power to lift heavy weights,

and power comes from muscles, therefore an ability,

and awareness is the power to say “I am”

and “I am” comes from the brain (without a brain, I would be dead),

then awareness is an ability.

Of course, trained philosophers will not be satisfied with this, nor should they:

“Ok, but is awareness really just an ability of the brain? In my opinion, you are treading dangerously close to the reductionism fallacy!”

This is a fair critique. But let me be clear: I am not claiming that awareness is nothing but distinction. Rather, I have discovered that distinction is the base layer upon which the whole concept of awareness rests.

On top of distinction, we can now place “knowledge or perception of a fact or situation.” In this way, the complexity of awareness builds upward, but its irreducible foundation remains distinction.

This clarification allows me to see more clearly: distinction is not a sufficient explanation of awareness, but it is a necessary condition for life.

As for the question of whether awareness is truly an ability of the brain, well, to answer that, I must dig below life itself.

But before I do, let me invite you into a simple thought experiment:
Take two identical grains of sand. Place them side by side. Now ask yourself: what is the difference between them?

If they are truly identical in shape, weight, texture, and composition, then what separates them?

The answer is not found in their physical properties but in their position. One is here, the other is there. Their difference is a boundary.

This smallest possible distinction, “this, not that”, is what allows us to tell them apart. Without such distinction, there would be no difference, no perception, no awareness.


## *Chapter 4 - The “is” in everything else*

As it appears with life, its existence relies on a bounded, finite form, a within and a without, through which the self becomes a unity. This act of differentiation, which I have called distinction, is what allows life to recognize itself.

But is distinction equally important for non-biological existence?

To even attempt this question, we must recall how this journey began. I argued that nothingness is impossible, therefore existence is necessary. Without this starting point, we risk falling into confusion, asking questions without foundation. 

Immanuel Kant, the legendary philosopher, gave us the confidence to admit that things exist outside the perception of the human mind. He asked: “What can we know, and how can we know it?” 

His answer divided existence into two complementary notions:

The noumenon – that which exists independently of perception, beyond the reach of our interpretation.

The phenomenon – that which we observe, measure, and agree upon.

Kant concluded that the noumenon is unknowable, though it must exist.

This, in my opinion, is an essential insight. And I take it one step further: we do know at least one thing about the noumenon, it is not nothing.

Why is this important? Because for a noumenon to become a phenomenon, it must acquire a form, a differentiation, a boundary through which it can step into the realm of the knowable. Yet even before it is noticed, before awareness draws the line that makes it distinct, it must already exist.

Let us now turn to the smallest known entities of the universe: the matter particles.

Matter particles are the fundamental building blocks of all ordinary matter. You, me, a stork, a rock, an atom, a planet, even our galaxy, all are made of these tiny units that, together, give substance to the universe. Matter is the raw material through which existence shaped itself into form.

But what fascinates me most about these particles is this: they are distinct.

They do not melt into one another when forming a rock. Instead, they hold together, side by side, bound yet separate. And when many of them join in patterns, they bring into existence everything we see and touch.

It would appear, then, that the very nature of the universe carries distinction at the base of its own existence.

If matter particles are distinct, then matter is distinct.

If matter is distinct, then everything composed of matter is distinct.

If humans are composed of matter, and everything composed of matter is distinct

Then humans are distinct inherently.

Matter itself is built upon distinction. Particles do not dissolve into an undifferentiated whole. Each exists as a bounded unit. From this foundation, larger structures emerge such as atoms, molecules, cells, bodies and each preserving distinction while layering complexity. Humans, too, are distinct, not merely because they are made of distinct particles, but because the very possibility of their existence depends on matter’s irreducible separateness. Thus, our awareness of being distinct is not an illusion, it is the echo of the universe’s own architecture.

*Let us be clear and honest: I am not suggesting that we are composed of atoms in the same static manner that a heap of sand is composed of grains. Since this is a philosophical treatise, this is my own method of visualizing how we are composed, shaped, and held in our present form by matter through the Principle of Distinction.*

*I do not wish to contradict the empirical data that points to DNA, the role of electromagnetic fields, or the constant flux of electrons between atoms. I am neither a physicist nor a biologist; therefore, I cannot claim to explain in technical detail what science conveys with such precision and accuracy. However, I do claim to understand the teleology of these scientific findings. It is from this understanding, of what matter does and what it forms, that I have derived the Principle of Distinction.*

*I am not proposing a static “pile”, but a specific organization of energy. While atoms are the “stuff” of the heap, it is the persistent pattern of their interaction that constitutes the Distinction. My focus is not on the “grains” themselves, but on the sovereignty of the boundary they create when they act in unison. I have done my best to remain faithful to the empirical foundation of reality while keeping this treatise firmly philosophical, focusing on the boundary where matter becomes a Self.*

Now imagine a small human forming in the mother’s belly.
When the brain has developed enough to activate, what is the first thing it does?

It tests its boundary.
It begins moving the body, the hands, the arms, the legs. This is the moment it explores its within. 

The second act follows naturally. After mapping itself, it begins to reach outward, to explore the unknown and sense the environment. And this is the moment it sets itself apart from the surroundings. 

We can observe, then, that the brain’s first achievement, once it is evolved enough to act, is to establish a relational recognition between itself and the environment.
In this moment, two facts become clear:

The brain is made of matter, which is inherently distinct. 

The brain’s first act is to distinguish itself from the environment.

The concept of awareness solidifies into reality through its most basic foundation: distinction. It becomes observable as a recurring pattern in the behavior of fetuses within the womb, once the brain has developed enough to act. In this patterned exploration of boundaries, we can trace the inheritance of distinction itself. Awareness, then, is nothing less than the brain’s ability to set itself apart from its environment.

Therefore, distinction is the means through which the brain gains the capacity to set itself apart from the environment, as awareness. With time awareness becomes evolved enough to be able to loop inwards and give rise to consciousness, and see itself existing.

## *Chapter 5 - I think, therefore I think about Thinking*

This journey began with the impossibility of “nothing” and has led us to the necessity of existence. We noticed how language shapes our lives and steers our very thinking. I claimed that we can address part of the fuzziness of language by recognizing that purpose is not a universal currency of achievement valid for all. It is now time to examine that claim more closely.

To begin, we must realize that what we take for granted always carries a hidden debt to reality. And to uncover that debt, we must dare to question the very act of questioning itself: what is thinking?

Cognition, after all, is not a gift exclusive to humans. It is an ability of the brain, yes, but also a spectrum of life itself. Even the simplest cell endures across time as a separate entity, distinct from the countless others around it. This primal distinction, this self-versus-world boundary, is the seed of cognition.

**Google defines cognition as:**
*“The mental process of thinking, knowing, and understanding, encompassing activities like perceiving, learning, remembering, judging, and problem-solving.”*

This definition does capture the essence: when a thought arises, an entire ecosystem of processes unfold in sync and harmony. But the inevitable question still rises: **what produces thought itself?**

Here is a small thought experiment:
*Have you ever tried, with full effort, to think of something that does not arise from memory, past experience, or even from the present moment?*

The instant you accept this challenge, the paradox emerges: your attempt has already been nudged into being. The very act of trying to generate an uncaused thought becomes its own cause.

So, if we observe carefully, the cause of thoughts is always linked to external events as they are perceived. This means that thought generation is neither random nor spontaneous. Even in the midst of work, when the mind drifts away, the drift is never without anchor: it is grounded in memory. Even imagination, which feels like pure invention, is fueled by past events: a book read, a film watched, a game played. And when we picture hypothetical scenarios, they too are stitched together from the faces and voices of real people in our lives. Wherever we turn, the source of thought is found in lived reality.

From this I am ready to make a new claim: thoughts are produced by the mind when an external input presses against the self, is perceived, shaped by language, stored in memory. And furthermore, **thought generation itself is an ability of the brain.**

And of course, let us test this claim, for as I have shown before, all claims must be attached to arguments.

Language, or better said speech, is a what we start gaining since early childhood, a skill we then refine across time to communicate with others. It is obviously an ability, one that can be strengthened through use or weakened through neglect.

Memory is likewise an ability: the capacity to store information across time, never perfectly, but well enough to be recalled and reshaped later.

Now imagine this: if we remove the ability of speech, if we strip words from the mind, and at the same time erase memory itself, how could one produce a thought? It would be exceedingly difficult.

Try it yourself: attempt to form a thought without using words. Then, on top of that, imagine you have no recollection of yourself at all, as if you just appeared here, now, without a past. The very idea strains imagination. Such a state would cripple thought generation.

This demonstrates that when abilities are removed, the “mental process” is greatly affected. And if the lack of abilities can impair thought, then thought generation itself is 
also an ability.

Of course, some may argue that thoughts do not always rely on words. After all, we can picture an image, recall a face, or sense an emotion without putting it into language. But even here, the scaffolding is the same. A mental image is still drawn from memory, a face is remembered from the past, and even emotion is shaped by experience.

So-called “non-verbal thoughts” are not free of abilities, but rather they are simply thoughts expressed in another form. Memory still supplies their content, awareness still frames them, and consciousness still binds them together. 
I predict that at this point, what follows may appear radical. But as we have already trekked through the realm of the self, we observed how appearances often hide meaning in what we take for granted. 

Awareness is the perception of the environment, processed by cognition into memory and grounded in the inherent distinction of the self.

This persistence of awareness through time eventually gives rise to the moment in which the subjective, first-person experience or qualia becomes witnessed from within. Consciousness confronts us with the responsibility of ourselves.

We then saw how thoughts arise within the long chain of causality.

These layered mechanisms of the self, I have argued, are abilities. The foundation upon which we project our identity, how we see ourselves, and how we become unique individuals on a rock called Earth where billions of others also exist. In this vast sea of humans, every single individual remains distinct and truly unique. And precisely this distinction is what makes life meaningful.

Imagine this:
Take one grain of sand and place it on another, and another, until eventually you have a heap. Now pause and ask: at which point do you draw the line? When does a heap truly begin?

I urge you not to rush this linguistic paradox. Sit with it for a moment because in doing so, you join a long line of thinkers who have wrestled with it for centuries.

Though I’ll offer my view, take it not as a verdict but as a lens, one that may shift how reality appears.

The answer doesn’t lie in counting, nor in smuggling in new nouns like “group” or “pile,” which only change the question. I argue it lies in distinction: grain by grain, there comes a moment when the identity of the whole overrides the identity of the parts. When the distinction of the grains fades into the structure of a new form. That is when a heap begins.

Now let's zoom out and look at Earth as a global society. From above we see the achievements: cities rising, nations forming, technologies unfolding at an exponential pace. We see the whole, the heap.

Yet beneath it all lies the individual, the grain, each one adding, in their own way, to the level we now stand upon. It may appear that humanity achieved this by sharing a single, unified goal. But in truth it is the opposite. What we share are not identical purposes but our differences. It is our uniqueness that allows us to explore existence from countless angles at once. And in this diversity of perspectives, we begin to notice the patterns that bind everything together.

And still, there persists a tendency, formed steadily and slowly over centuries, to project a universal target across all individuals. We have given this tendency a single name: purpose.

## *Chapter 6 - Nevermore, never waking*

We project our own construct outward, stretching it across everything:

> - Inanimate objects are said to have purpose.
>
> - Animals are said to have purpose.
>
> - Humans are said to have purpose.

But since we have already seen how our mental abilities give rise to meaning itself, this does not imply that everything else carries the same necessity. It is convenient to say that the purpose of a plant is to recycle carbon dioxide into oxygen. Yet what we are really saying is different: the plant exists, and by the process of photosynthesis, oxygen results. The plant’s role in the ecosystem is undeniable, but that is not the same as declaring that the plant has a purpose.

Purpose, unlike mental abilities such as awareness, consciousness, and thought generation, is actually a construct we create to anchor the self within existence. It is the mode by which humans live with meaning.

And since each human is distinct, no thought is ever identical to another’s then by extension, neither is purpose.

Now pause and consider this: when we take a word whose very meaning is tied to the importance of the self, and we strip it of that diversity unconsciously, reducing it to a single formula of “success” then what consequences could we possibly expect?

It is obvious that universality cannot serve everyone. Yes, life may be easier with wealth, but is the pursuit of that wealth truly enjoyable? And when the pursuit does not seem to draw nearer, when the destination shifts further away because the system itself is designed in such a manner, can such a chase still be called meaningful?

I am not suggesting that we should stop seeking to improve the quality of life. Rather, this improvement should arise only when such a pursuit truly aligns with oneself in that specific moment. Growth need not be forced or a restless, continuous chase. It can unfold as the organic aftermath of stability.

If we understand that purpose belongs to who we are, rather than to what we are told, then the accumulation of wealth begins to pale in comparison to the exploration of the self. In that realization, we finally see what the pursuit is truly about.

And the most important truth we must internalize is this: we exist because existence itself is necessary. Our lives are not a cosmic gift, but our thinking is, and therefore, so is our individual purpose.

I would like to emphasize that this is my own understanding of purpose, shaped through personal exploration and reflection. One of the guiding threads that led me to this view, reaches back more than two millennia, into a school of thought that still has practitioners today: Stoicism.

Stoicism is not merely an abstract branch of philosophy but also a way of living. Founded around 300 BCE by Zeno of Citium in Athens, it has endured as one of the most influential and practical philosophies in Western civilization.

At its core, Stoicism teaches that life should be lived in harmony with the natural order. To live well, one must first recognize the difference between what lies within the control of the self and what lies beyond it.

Perhaps the most famous Stoic principle is this: our thoughts, judgments, attitudes, and choices belong to us, while other people’s opinions, external events, and even the past remain outside our power. The most rational way of living, according to the Stoics, is to focus all energy on what we can control and to accept what we cannot. 
To navigate this principle in practice, the Stoics handed down four virtues:

Wisdom: The ability to navigate complex situations and make sound judgments.  

​Justice: Treating others fairly and with kindness, and acting for the common good.  

​Courage: Facing adversity, fear, and pain with a steadfast mind.  

​Temperance: Exercising self-control and moderation in all things.  

To me, these Stoic virtues resemble nothing less than the description of a normal human being in today’s world. They form the essence of citizenship, of responsibility, and of the ethical backbone that holds society together. And yet, while these virtues have been internalized, woven into our sense of what “normal” ought to look like, we still often stray from such a way of life.

This raises a pressing question: why? What is the reason we drift from principles that seem so natural and so essential for a balanced mind?

I cannot claim to provide a final answer, but what I can do is recognize patterns.

Just as words evolve over centuries, shifting in meaning until they scarcely resemble their original form, so too have the teachings of the ancients been reshaped into new doctrines. The Stoics, for example, were determinists in practice, though the word “determinism” did not yet exist in their vocabulary.

The term “determinism” first appeared in 1846, when Sir William Hamilton described the doctrine that all events are determined by preceding causes. While determinism is not a direct descendant of stoicism, it was nonetheless influenced by it.

For the stoics, causality was inseparable from cosmology. The chain of cause and effect was understood as the unfolding of the divine logos, the rational order of the universe. Modern determinism, by contrast, is largely secular. It strips causality of divinity, presenting it instead as a chain of events, each event following from the last, without reference to any higher power or destiny.

Since 1846, determinism has branched and multiplied into numerous schools of thought, often overlapping and diverging to the point of obscuring its original essence. Today, with the full weight of scientific authority behind it, determinism has become not only a dominant framework but arguably one of the most widely practiced philosophies in the modern world.

As a response to this growing philosophical movement, and in search of a common ground between those who dismiss choice as an illusion and those who insist that choice is unrestricted by causality, compatibilism emerged.

Compatibilists argue that free will can indeed exist, but only within the confines of the universal causal chain. In other words, our freedom is not the absence of causation, but the ability to act in accordance with our own motives, desires, and reasoning, even if those, too, arise from prior causes.

Thus, what had long been a two way debate, determinists on one side and libertarians on the other, gained a third player. A centuries-old argument found itself reframed, not by denial or rebellion, but by reconciliation.

It would only seem obvious that compatibilism would change perspectives, and while its impact on philosophy is undeniable, this has not prevented the steady attacks on the concept of free will.

At first glance, it now appears that our understanding of the self has “evolved” into a reductionist view: that we are merely biological machines, and that there is nothing inherently special about the mind.

A very recent publication by Robert Sapolsky, a highly respected neuroscientist, primatologist, and professor at Stanford University, embodies this trend. His 2023 book, “Determined: A Science of Life Without Free Will”, systematically dismantles the notion of free will from a scientific standpoint. Drawing on decades of research in biology, neuroscience, and behavior, Sapolsky argues that every human decision is the inevitable result of prior causes: genes, hormones, neural activity, and environmental conditioning. In his view, what we call “choice” is simply the sum of influences acting upon us. In the second half of the book, however, he shifts from description to prescription: if free will is an illusion, then punishment for wrongdoing should give way to rehabilitation and prevention. The absence of free will, Sapolsky argues, is not cause for despair, but for a more humane approach to justice.

Another recent work comes from Sabine Hossenfelder, a theoretical physicist known for her sharp and uncompromising style. In “Existential Physics: A Scientist’s Guide to Life’s Biggest Questions” (2022), she explores several major philosophical issues: God, meaning, the multiverse, and immortality, through the lens of modern physics. On the matter of free will, she is as direct as Sapolsky: libertarian free will, the idea that choices are uncaused and unrestricted, is simply incompatible with the laws of physics. 

While I strongly recommend reading their books, I cannot help but notice a recurring pattern. Free will is almost always defined as the power of choice unconstrained by causality and then promptly dismantled using observable, testable scientific methods. On one hand, it is difficult to argue against their conclusions, the evidence is compelling. But on the other hand, one must ask: why enter the debate on their terms in the first place?

I do not deny their findings. I agree that our choices are, in some way, shape, or form, influenced, if not outright determined, by the past. 
But here is the deeper question I raise: how do we know that free will is equivalent to choice at all?

## *Chapter 7 - The end of the Chain*

In 1748, the legendary philosopher David Hume strengthened his position as a compatibilist with his “Philosophical Essays Concerning Human Understanding”, later revised in 1758 as “An Enquiry Concerning Human Understanding”. Hume tackled the tension between free will and causality by reframing the problem: a free action is not one that floats without cause, but one that flows from the individual’s own will and desires.

For Hume, freedom was meaningful precisely because it was tied to character. We hold people responsible for their actions not in spite of causality, but because their choices emerge from who they are. If an action were uncaused, it would be a matter of chance and could not be attributed to the person, making moral responsibility impossible.

He defined free will as “a power of acting or not acting, according to the determinations of the will.”

To summarize Hume’s insight: free will is not merely selecting from a menu of possible outcomes. It is the capacity to act with agency, in alignment with one’s distinct character, the irreducible self that makes each human being unique.

This tendency to dress up free will in libertarian clothes, only to be swiftly torn apart, persists even though the compatibilist view has long been on the table. What I have not yet observed is a genuine pivot of the debate toward freedom within causality, rather than freedom exempted from it. And so the discussion remains frozen, circling the same question of whether free will exists at all.

It seems to me that the issue lies in this very definition of freedom as being unbound by causality. The problem is that, even today, free will is still framed primarily as a causally unchained choice. What was once a philosophical question, today has been reshaped into a scientific dismissal of a dualist mysticism, precisely because science can show with confidence that every choice, thought, judgment, attitude, and even character itself is determined by prior causes. Yet in devoting so much attention to disproving free will as unconstrained choice, we risk missing the necessary question: what gives us the certainty that free will is the power of choosing?

This definition of free will is what we take for granted, much like the assumption that education automatically leads to a well-paid job. It is also one of the reasons compatibilism is so often nudged aside in debates, while attention is directed at the easier target of libertarian free will. Yet I believe these two concepts, free will and choice, cannot be equated. The situation mirrors the confusion between awareness and consciousness: two distinct layers collapsed into one. 
But in the case of free will, the consequence is greater than confusion for it diverts the entire discussion toward dismantling a strawman.

Daniel C. Dennett, the man, the myth, the legend. Philosopher and cognitive scientist, offers one of the most compelling compatibilist accounts of free will in his book “Freedom Evolves” published in 2003. He argues that freedom is not a mystical quality but an evolved trait, fully compatible with a deterministic, scientific worldview.

Dennett’s central thesis is that free will is a complex set of abilities that emerged through natural selection. As organisms grew in complexity, they developed greater degrees of flexibility in behavior or what we might call “degrees of freedom.”

A key part of Dennett’s argument is the distinction between determinism and inevitability. Even in a deterministic universe, where every event is caused by prior events, the future is not fixed in advance. Our ability to anticipate, evaluate, and avoid certain outcomes grants us precisely the kind of freedom that is, in Dennett’s words, “worth wanting.”

In this way, Dennett not only strengthens compatibilism but also stabilizes it, showing how free will and causality can coexist in harmony. If free will is understood as a mental ability, then choice is not identical with free will but its precondition, another capacity of the highly evolved brain.

Consider this thought experiment:
You find yourself in the ice cream section of a shop. 
There are four flavors before you: chocolate, vanilla, strawberry, and banana. 
You know with certainty that you want ice cream, and as soon as your eyes meet the freezer, something subtle occurs in your mind. For the briefest instant, all four outcomes exist together and each a live possibility. Then, almost immediately, your cognition performs a rapid preference check and collapses that wave of possibilities into one outcome. Of course, curiosity or mood might nudge you toward a new flavor, but more often the decision settles according to habit, environment, or personal taste.

The important insight here is that, however briefly, we do entertain multiple possible choices before one crystallizes into action. And even after the fact, we reflect on whether the choice aligns with the moment, before we actually act on it. 

Of course, the power of choice is not unique to humans or just adults. We can clearly observe it in many animals, and also in children as young as one year old. Different dogs show preferences for different types of food, cats decide when (and if) they will accept being petted, and toddlers often choose the company of one parent over the other.

The complexity of choice is directly proportional to the size and sophistication of the brain from which it arises. Choice, then, is indeed an ability. It is the capacity to collapse a wave of possibilities into a single outcome. 

This capacity stands upon the foundation of distinction, present in all the levels of the universe and in all life from the simplest organism to the most complex. Without the ability to be set apart from the environment, choice could not arise at all.

Consider infinity. Infinity stretches without end, and regresses without beginning. It is the very antithesis of distinction. In infinity there is no boundary, no contrast, and therefore no change and no evolution. 
Imagine a white sheet of paper extended infinitely in all directions: it becomes pure, undifferentiated whiteness, a stillness so absolute that nothing can ever occur within it.

The only reason a white sheet of paper is truly a sheet of paper is its limit. Because it has edges, it can be altered, reshaped, or destroyed. Its form can be engaged. In the same way, choice requires boundaries. Without limits, there is no “this” or “that,” no path to take, no alternative to resist. Choice is born not from endlessness, but from finitude.
Without boundaries, all would dissolve into sameness.

I believe this is the right moment to recapitulate the hierarchy uncovered so far in our exploration of the self.

It begins with the most fundamental act, distinction. From distinction arises awareness, which, when carried forward in time, gives rise to cognition. Upon cognition memory builds. From memory flows language, which compresses identity across time into symbols. With language and memory intertwined, the mind acquires the ability of thought generation. From thought emerges choice, the ability to navigate between distinctions. Out of this layered structure arises sentience, the ability to have feelings and emotions. And finally, at the highest rung, consciousness develops which is, in its irreducible form, the subjective experience of introspection.

Remove even one of these abilities, and consciousness collapses. The tip of the ladder loses its essence the moment any rung beneath it disappears.

In this light, it becomes clear that awareness, sentience and consciousness are not synonymous. Awareness is the starting point of life’s unfolding and consciousness is its evolutionary culmination. They are connected by necessity, but never identical.

Think of this commonplace example: *“Mary is feeling cold.”*

At first glance, this appears to be a simple description of discomfort, the body reacting to an environment where the surrounding temperature is lower than its own. We routinely use constructions such as: feeling cold, feeling hot, feeling pain, as if they were straightforward. 

Yet what does the verb “to feel” truly mean?
Dictionaries assign it a wide range of uses: the sensation of touch, the experience of emotions, states of mind, or even the holding of a belief or impression. It can denote the perception of one’s inner state as well as the recognition of external conditions. But beneath this linguistic versatility lies a more precise process.

Mary is a finite life form, with an interior and an exterior. Her sensory organs register that the surrounding temperature is hostile. The cold presses against her body, threatening her internal equilibrium. Signals flow to the brain: one reporting the environment, another reporting the body’s loss of heat. The brain, endowed with cognitive functions, correlates these inputs and recognizes a causal link: the outside is disturbing the inside. The thought arises: “It is cold.”

At that moment, the boundary between mere sensation and action is crossed. Mary begins to seek relief: to find shelter, to add clothing, to protect herself. She does not choose to feel the cold, that reaction is imposed by her environment. But if she refuses to act, if she deliberately faces the cold rather than escaping it, then reaction transforms into choice. 

It is essential to distinguish between reaction (the cognitive process of turning input into preservation) and choice. When we take words for granted, as in the case of the verb “to feel”, we begin to confuse reacting to a context with experiencing life. At first this may not seem problematic, yet the consequences lie hidden in plain sight, buried in what we take for granted in our routines. 
If we lose the ability to separate impulses from choices, we risk forgetting that we possess intention and the capacity for deliberate action. And when language is allowed to drift unchecked, when words like to feel, purpose, or even education are stretched into distortions, we surrender the very tools that allow us to identify what makes us distinct as human beings, and unique as individuals in society. 

To make my critique of the everyday use of “to feel” more concrete, recall that in the ladder of consciousness I placed sentience above choice. I argue that it is “choice” which gives rise to emotions and feelings, not the other way around. 

Returning to Mary: if she deliberately continues to endure the cold without protection, frustration will inevitably follow. At some point she will ask herself, “Why am I doing this?” and that very question represents another choice, a choice to abandon her rebellion against causality. She gives up not because the cold itself compels her, but because she feels annoyance at her own decision to suffer. When she finally acts to protect herself, she experiences relief and even happiness at having surrendered to self-preservation.

This is why I regard our casual use of “to feel” as reckless. The verb should be reserved more narrowly for the arising of emotions, feelings, and states of mind. Greater precision in language would allow related synonyms to gain a proper hierarchy of meaning:

To perceive, to sense, to detect - the most basic rung, describing the brain’s cognitive reactions to input. 

To notice, to observe, to identify - the recognition of patterns in the environment or in behavior.

To discern, to make out - the beginnings of reasoning, where raw input is interpreted.

To endure, to bear, to suffer - the confrontation with one’s own condition, where meaning begins to take shape.

To feel, to experience - reserved for the higher expression of emotions and states of mind.

Such a hierarchy not only sharpens our language, but also safeguards our understanding of what it means to act, to intend, and to feel in the fullest human sense.

The connection between free will within determinism, the compatibilist view of evolutionary agency, and the impact of this debate on our daily routines can be traced back to language, more specifically, to the words and symbols that generate systems. The brain internalizes these symbolic systems, shaping the very container in which thought arises. Systems such as mathematics, physics, and biology are widely accepted constructs, providing us with a framework that can be inherited, modified, and expanded across generations.

Consider the case of engineers who can examine a bridge entirely in their mind, rotating it, dissecting it, and analyzing its structure without uttering a word or invoking a single symbol. This kind of purely visual reasoning is rare, but not mysterious. What enables it is the deep internalization of a system of rules and laws refined over centuries of accumulated knowledge, trial and error, and practical success. To the outsider, it might appear as an innate gift; in reality, it is the product of relentless study, memorization, and the cultivation of a symbolic framework. And it is crucial to remember: that framework first entered their minds through language, more exactly, through words and symbols.

I now turn to philosophy, a discipline older than science, yet also a system, one that operates not in the tangible but in the abstract. It is a framework devoted to analyzing thought itself, and it is utterly dependent on words and inevitably shaped by subjective interpretations of their meanings.

Unlike mathematics, physics, or biology (fields that demand precision in their definitions and symbols) philosophy is unavoidably open to reinterpretation. Its concepts and norms shift with context, culture, and era, because at its foundation lies thought itself: fluid, recursive, and impossible to pin down with finality.

Precise systems such as mathematics, physics, and biology are largely insulated from the shifting meaning of words. Their empirical nature anchors them, so that a theorem or a law holds regardless of cultural change. Philosophy, by contrast, is ever-changing, carried along by culture, and the centuries themselves. Language shapes philosophy, and philosophy, in turn, reshapes language.

As a result, we encounter concepts that we experience directly yet cannot articulate with accuracy. Where precision fails, interpretation takes over. And when we interpret, we generate new concepts that themselves invite further interpretation. 

Free will is one such case and its interpretations quickly seep into questions of morality. Is morality inherent to human beings, or could it exist in other forms of life? Is morality a subjective construct, or does it stand as an objective feature of reality?

I believe we now hold enough of the puzzle pieces to approach what free will might truly be.

It is clear that free will cannot be reduced to the mere ability to choose between a variety of outcomes. That capacity belongs to choice itself, which must therefore be treated as the precondition upon which free will stands.

We also have sufficient grounds to move decisively away from the libertarian view, which imagines freedom as an uncaused spontaneity. Compatibilism, by contrast, situates free will comfortably within the framework of determinism. Through the long arc of evolution, the ladder of consciousness has unfolded step by step, and this very structure becomes the necessary groundwork for free will.

Thus, we may tentatively define free will as the culmination of the evolutionary refinement of the human brain, the most intricate cognitive system known on this planet. Yet even here the central question remains unresolved: what
is free will?

Everything we have explored so far offers clues to unveil the answer to this enduring question.

Science speaks with clarity: physics tells us that free will unconstrained by cause is impossible, while biology offers compatibilism as a plausible answer, showing how choice emerges as a structured product of evolution and environment.

Religion, in contrast, frames free will as the capacity to choose between obedience and sin.

“I call heaven and earth to witness against you today, that I have set before you life and death, blessing and curse. Therefore choose life, that you and your offspring may live.” (Deuteronomy 30:19)

“If you love me, you will keep my commandments.” (John 14:15)

The religious stance, then, is that although it is easier to act in one’s own self-interest, true freedom lies in aligning oneself with the word of God and in acting selflessly.

When science and religion are placed together, like pieces of a puzzle, a pattern begins to form. Free will manifests itself not in grand metaphysical assertions, but in the smallest of everyday acts.

Consider a person, already late for work, rushing to exit their building. As they open the door, they see an elderly woman moving slowly in their path. Instead of pushing ahead, they pause, wait patiently, and even offer her a kind word.

Or imagine the destitute man waiting by an ATM, asking strangers for spare coins. Though hunger presses on him, he does not turn to theft, though it might secure him food for days.

In such moments, we see that free will is not simply a matter of “choosing.” It is the point at which consciousness resists the chain of causation. It is where we impose ourselves against the most obvious outcome, an outcome that would serve only our own needs, and instead redirect choice toward the well-being of another.

It becomes clear, then, that free will is the capacity to resist the most obvious outcome, and this resistance shines brightest where self-interest collides with the well-being of others, or with a conceptual internal value. 

In a deterministic system, the “most obvious outcome” is the predicted effect rising from causes.
​Free Will is the evolved cognitive function that allows the self-aware agent to override that prediction and impose a new vector against the default, evolutionarily determined outcome. 

The journey from the impossibility of “nothing” has shown that existence is a necessity, and our individual self is defined by its distinction from all else. This distinction is the bedrock upon which choice arises. And choice, refined by consciousness, culminates in free will: the conscious, effortful capacity to resist the most obvious outcome in favor of an action that honors the broader system of which we are a distinct part. 
Our daily struggle, then, is not against an illusory fate, but against our own impulses. 
We are not machines “programmed” to react, but free agents designed to resist. 
This realization reveals the true function of the word “purpose”: it is not an external goal of wealth or status, but the internal, continuous practice of imposing that ethical resistance. 
Our purpose is the freedom we choose to exercise.

## *Chapter 8 - I am complete at Last*

It is commonly held that the self is fully described by consciousness and the contents it sustains such as thoughts, feelings, memories, and awareness. I, however, have reached a different conclusion. The self is not merely consciousness, but the entire ladder that gives rise to it: from distinction, to awareness, to perception, to cognition, to purpose, and ultimately to free will.

We, human beings, are the only known entities capable of transcending the causal chain while still remaining within it. Through the evolved sophistication of our cognition, we can impose a path more complex than the one causality alone would determine. It is this ability to redirect, to resist, and to reshape the course laid before us, that completes the self.

It is not some mysterious link to a higher dimension that gives rise to our free will. It is time itself, the slow accumulation of centuries spent questioning our existence, our choices, and our place within the world. Through this long search for understanding, humanity has refined the very capacity it so often doubts. Our laws, our rules, and our systems of justice did not bloom suddenly into being. 
They are patterns, recurring manifestations of what happens when humans gather, deliberate, and shape a shared purpose. It is the collective realization that there is strength in numbers, and that cooperation is the highest expression of the will to endure.

I am certain that we do not obey laws merely out of fear of punishment, but because, over time, humanity has understood that preserving all human life is far more beneficial to each individual than preserving only one’s own. We have developed an intrinsic reasoning to help one another, not solely because religion instructs it, nor because our laws demand it, but because we have witnessed what happens when isolation prevails.

A lone human in a dark forest may tremble before the unknown, but that fear diminishes as companions gather. In unity, courage takes form and facing danger together grants us the confidence to keep moving forward. This instinctive value for life, however, is not unique to humanity. Across nature, countless species form societies and establish rudimentary rules to endure, from ants and bees to wolves and whales, and ultimately, to us. Cooperation is the natural evolution of survival, but we, humans, do take this cooperation further. 

Why do we teach our children to eat with their mouths closed? Is it merely because the sound annoys us, or because the sight of chewed food is unpleasant? Or is there something deeper at work? When a child asks, “Why do I need to eat with my mouth closed?”, the answer comes easily: “Because it’s not nice.”

We accept this answer without much thought, yet beneath it lies something profound. Eating with one’s mouth open is not simply poor etiquette, it is a small window into what separates us from animals. It requires more effort to chew and breathe with the mouth closed, but we do it because we have evolved into complex, self-aware beings who understand the comfort of others. Rules of etiquette, subtle as they may seem, are reflections of empathy. They are the quiet proof that humanity not only values life itself, but also the experience of living together. 

When we trace our behavior from the simplest gesture of politeness to the grandest moral decisions, a single thread runs through them all: awareness of the other. Every act of restraint, every moment of patience, every instinct to protect or comfort another life, these are echoes of the same structure that began with the impossibility of “nothing”. From distinction came awareness, from awareness came empathy, and from empathy, civilization itself. The completion of the self does not lie in isolation or supremacy, but in the recognition that to preserve others is to preserve ourselves.

When we look at us from this angle, we can clearly see that free will, the individual’s ability to resist causality, becomes the very precondition for morality. Through this resistance, we transcend the self for the benefit of others. Hence, morality is the collective expression of free will: the shared mechanism we have evolved not merely to coexist, but to value and appreciate one another.

--
## Chapter 9 - Practice makes perfect 

Were you expecting this rambling to finally end?
Did I deceive your expectations?

Well, I’m not sorry if I did, because now we arrive at the point where the “ought to” must face reality.

This chapter marks the moment when the ladder I boldly call “the ladder of consciousness” meets the world it tries to describe.
Here we test the framework against the lived concepts that shape human life: morality, ethics, altruism, sacrifice, guilt, kindness, the need for laws, the demand for justice.

It is my conviction that if a framework built from first principles can account for the structures of reality without needing to invent additional layers or auxiliary systems, then it stands as a valid and coherent framework.

So let us recall the ladder beginning from the premise that existence is necessary:

Distinction → Awareness → Cognition → Memory → Language → Thought → Choice → Sentience → Consciousness

This ladder culminates in the emergence of free will, the capacity of the most sophisticated minds to transcend the most obvious deterministic outcomes and now we apply it.

Morality has always posed difficulties in philosophy.
Although we possess the common-sense ability to distinguish between right and wrong, it is precisely the origin of this sense, so universal, so immediate and so unquestioned, that we seem to lack knowledge of.

Why are we able to tell what is good and what is bad?
Is our moral behavior merely obedience to external rules?

For instance: do we follow laws only because we fear the consequences of breaking them?
If the answer were yes, then morality collapses into a simple equation: “Stealing is wrong only if you get caught.”

But that conclusion rings false to almost everyone.

Consider another problem: why should we respect the right to live of someone who did not?
If a murderer has violated the most fundamental right of another human being, is imprisonment truly the adequate moral response?
Should one who disregarded life still be entitled to the protection of society?

These are the kinds of tensions morality presents, questions whose answers we seem to “feel” more than we explicitly understand.
Our grasp on them exists, but mostly because of lived experience, cultural inheritance, and historical memory.

Have you ever noticed that when you ask anyone, even a stranger, “Is killing justified?”
Their first instinct is to clarify what you mean by “killing.”

If the answer is animals, most people reply that it can be justified, usually with conditions: because we need to eat, because survival requires it, because moderation and respect should guide it.

But if you press further “How about humans?”, the response changes instantly.
The answer becomes swift and unwavering: “No. Murder is not justified.”

This pattern is striking.
Across the entire planet, every society: in the Americas, in Europe, in Asia, in Africa, has laws that protect human life.
Every justice system defends the individual’s right to live.

This pattern reveals a global consensus: preservation of human life (and, by extension, life in general) is a universal moral instinct.

In other words, humanity has taken the instinct for self-preservation and extended it outward, granting the right to live not only to oneself but to all peers within the human community.

How do we explain the fact that the vast majority of human beings on this planet instinctively believe that life must be protected, cared for, and nurtured?

The answer follows naturally from the framework we have built so far.

If free will is the evolved cognitive capacity to overcome the most predictable impulse, then morality is this capacity extended outward, from the individual self to the surrounding community, or in other words, morality is the collective manifestation of free will. 

Human beings value life because each person values their own life.
But consciousness (the ability to reflect on oneself) and free will (the ability to transcend selfish immediacy) combine to produce something extraordinary:
the capacity to care for another human being enough to respect their right to live as well.

Morality, in this sense, is not a commandment nor a cultural invention.
It is the natural outward expansion of a self-preserving consciousness that has learned to see itself in others.
This outward extension of self-preservation forms the root of kindness, altruism, sacrifice, and justice.

We can be kind to one another because we are capable of being kind to ourselves.
The standard we hold for our own well-being naturally extends outward: first to our families, then to our friends, our colleagues, and even to strangers.

We are not polite because we fear punishment, we are polite because we respect ourselves enough to desire the same respect in return.
Courtesy is not obedience, it is reciprocity.
It is the outward reflection of an inward dignity.

And when we sacrifice our time and effort in service of another person, we are demonstrating something remarkable:
that we have evolved beyond the narrow limitations of deterministic self-interest.
We choose, freely, to value someone else’s well-being alongside our own.

Perhaps this has happened to you, or to someone close to you, or you have simply heard of it.
A cashier gives too much change, the customer notices the mistake, and instead of turning away with an unearned gain, they stop, correct the error, and return the extra money.
This small and trivial gesture is one of the clearest everyday manifestations of free will and morality.

No one would notice if the customer kept the extra coins.
It would be an effortless benefit, acquired without consequence.
No one would know, except the customer themselves.

But when they choose to return the money, something profound occurs:
they resist the immediate, obvious, self-serving option and instead relinquish the advantage.
They do so because they recognise it would harm the cashier.
They understand that the cashier might be held responsible later, might face trouble for a mistake they did not intend.

By returning the excess change, the customer shows that they value the well-being of another person, even a stranger, enough to correct an error that no one else would ever see.

This is morality in action.
This is free will overcoming initial causality. And yes, Free Will is also caused. 

Of course, this situation can be analyzed and interpreted in countless ways.
For example, a professional philosopher, cultivated in the works of legendary thinkers such as Friedrich Nietzsche, might push back and argue that the customer did not truly act with the cashier’s well-being in mind, but rather from a subtler form of self-interest. In this view, returning the extra money is not an expression of altruistic free will at all, but a way to satisfy an internal desire to feel noble, to reaffirm a moral self-image, or to uphold values absorbed from society since childhood.
From that frame, the action is performed not for the other, but to service a morality instilled by the “herd.”

This counter-argument is, on the surface, devastating to my definition of free will. It seems to turn my entire concept of “resistance to the most obvious outcome” on its head once the subjective experience is brought into the picture. And I will admit: I have felt this myself. I have helped someone and immediately thought, “That is my good deed for the day,” as if goodness were a quota and morality a daily chore imposed upon me. Nietzschean critique is sharp precisely because it exposes this internal psychological framing.

Yet the devastation dissolves the moment we examine what my definition was actually meant to describe.

Resistance to the most obvious outcome does not attempt to capture the subjective interpretation of the act, because it was never intended to. My definition of free will, which I will henceforth name “Nilogist Free Will”, describes the mechanical architecture of how such decisions arise at all. It explains why the subjective desire “to feel noble,” or “to be a good person,” exists in the first place. It identifies the structural process through which virtue emerges, rather than collapsing into simplistic deterministic impulses.

Let us apply the Ladder of Consciousness.

The customer is aware of their boundaries, because they are finite beings with a within and a without.
Perception registers the cashier as another distinct form; memory stores their presence; cognition acknowledges that this person is a separate identity, another human subject. Language and memory intertwine under the pressure of cognition, generating thoughts such as: “This person must feel as important and unique to themselves as I do.”
Possibilities emerge when cognition recognizes the mistake: an internal tension arises “If I keep this money, it benefits me” ,“No one would notice.”
Sentience amplifies this tension as emotional excitement.
Self-observation, or consciousness intervenes: “But would that action reflect who I am? Do I need this money that badly?”

Then free will emerges and it collapses the field of possibilities not toward the easiest gain, but toward the outcome that aligns with the preservation of another’s well-being
“They will be missing money at the end of the day. They may have to repay it out of their own pocket.”
And so the action crystallizes:
“Sorry, it seems you have given me too much change.”

How the subjective experience narrates this decision is entirely irrelevant.
This subjective narration, in my opinion, occurs not as the cause of the action, but as a post-hoc reinforcement mechanism, in other words, a way for the mind to store, justify, and ultimately reproduce the same outcome in the future.

Thus the important fact is that the actual outcome favored the other rather than the self.
And this resistance against the effortless, deterministic impulse is precisely the Nilogist expression of free will.

Now comes the moment to face the unforgiving reality.
The customer–cashier example is a common scene in daily life, one in which the stakes are negligible. Because nothing crucial is threatened, resistance to the obvious deterministic outcome is easily achievable. But if the nilogist concept of free will has any objective value, it must also predict the conditions under which resistance will fail.
To test this boundary, let us imagine a thought experiment that blurs the line between self-preservation, morality, and ethics.

Consider the following scenario:
A burning building. Inside are two people.
One is your child.
The other is a stranger, a scientist who, with absolute certainty, holds the key to curing cancer. You can save only one.

Do you save your child, allowing the scientist to die and potentially letting millions perish from the disease?
Or do you save the scientist, sacrificing your own child and living with the irreversible consequence of that loss?

I do not know how most people would respond, but I know how I would.
And I will admit it without hesitation:
Saving the scientist would not even appear as a viable option.
When the choice is between my child and a stranger, regardless of the stranger’s potential global impact, my child eclipses every other consideration.
In truth, I would not be “resisting” any impulse at all. I would fall far below the nilogist standard of free will.
There would be no internal debate, no collapse of possibilities, no triumph over determinism.
Saving my child is not a choice, it is an inevitability.

But this does not mean that nilogist free will is illusory.
Rather, it reveals that resistance to determinism is the most difficult achievement imaginable, especially in situations involving the survival of the self or one’s kin.

Why?
Because the Ladder explains it clearly:

We first recognize ourselves, and our own importance.
From this center, importance extends outward:
first to our immediate kin, and only much later, also much weaker, to strangers.
It is easy to be kind when kindness costs nothing.
But as the cost increases, the ability to resist deterministic selfishness begins to fade.

And for a proper reason:
If resistance were universally easy, if free will were available in every situation without strain, then “free will” would reduce to a trivial concept, indistinguishable from simple choice, exactly how mainstream philosophy currently treats it.
In such a case, free will would collapse into nothingness, losing the very distinction that allows it to exist.

It now becomes evident that resistance to causality, the highest and most complex mental capacity evolved by the most advanced brain we know, reveals that determinism is not merely a property of existence, but the natural manifestation of existence itself through its finite, bounded units of distinction.
In other words, determinism is the fundamental mechanism by which distinction stabilizes into higher-order structures. Without deterministic interaction between finite forms, no boundaries could persist, no identities could emerge, and no ladder of consciousness could ever arise.

Thus, causality is not merely a description of a physical law, as physics and mathematics frame it, nor simply a philosophical doctrine describing the relation between what was and what is or cause and effect.
Causality is, in my view, is the architecture of our perceived reality.
But by itself, this architecture is insufficient to account for the existence of free will.

Likewise, determinism, being the rule-based structure governing finite distinctions, cannot fully explain consciousness or the free will that emerges from it, because these arise at a level of complexity that surpasses the explanatory power of simple cause-and-effect chains. To be even clearer:
determinism gives rise to complexity, and at sufficient depth, complexity develops the capacity to resist the very determinism that produced it.

The strict determinist will vehemently reject this elevation of determinism from a mere law to the manifest architecture of existence. They will take one look at the nilogist definition of free will, resistance to the most obvious deterministic outcome, and dismiss it immediately. In their view, the effect (free will) is nothing more than the inevitable sum of all prior causes: genetics, environment, and conditioning. Resistance, they claim, is merely an illusion, a narrative invented by the self.

But when we examine this claim more closely, it collapses under the weight of the very simplicity the strict determinist prides themselves on.

If the effect is nothing but the sum of prior causes, then this denies the relationship between cause and effect at a fundamental level. Because according to this view, there is no meaningful difference between the cause and what follows it. The effect adds no new variable, undergoes no transformation, and does not represent a transition into a new state of distinction. Everything is simply inevitable, predetermined in a frozen chain of static events.

Yet this position contradicts determinism itself.
If there is truly no difference between cause and effect, then the causal chain ceases to be a chain at all. It becomes a static, timeless block, a universe in which nothing actually happens. Determinism becomes indistinguishable from stasis.

In my view, for a cause to be a cause, it must initiate a transition from one state of existence to another, or better yet, a transformation of distinction. And the effect must represent this new state, one that differs meaningfully from the starting point.

This is why causality cannot be reduced to inevitability.
Determinism describes the mechanism of time itself: the fundamental difference between what was and what is, the transformation of distinction from before to after, and the unfolding of entropy as the measure of possibility. Thus the causal chain is the architecture of existence, not a script of inevitabilities.
Therefore, it is irrelevant how we perceive ourselves or narrate our lives, and the factual reality is that our actions shape our own environment. 

## Chapter 10 - Application, Application… Application?

Throughout this exploratory text, I have used the word “environment” countless times, but have I used it according to its accepted mainstream meaning?

I claimed that something is not “nothing” because it is distinct from the environment. Yet, according to Wikipedia, the definition of environment is:
> “Environment (surroundings of an organism or population) means what surrounds us. It may consist of living or non-living things. It includes physical, chemical, and other natural forces.”

If this definition is applied to my usage of the word within the context of the text so far, it becomes clear that it makes no sense whatsoever.
This means that I have fallen into the very behavior I so often criticize. I used the term “environment” with an assumed meaning known only to myself, while simultaneously expecting the reader to already understand what I meant by it.

This is precisely what I mean when I say that fuzzy language affects our lives more than we care to admit.

For this reason, many debates consist of people talking past one another, using the same lexicon, yet attaching different meanings and definitions to shared vocabulary. Disagreement then arises not from genuine opposition, but from confusion. It is not only important terms such as consciousness or free will that require shared definitions before a debate can be meaningful. Equally crucial are the most common words, the ones overlooked because of their apparent obviousness, such as “environment.”

Before I expose my hidden assumption regarding “environment”, I must address one additional point. Philosophy is undeniably best known for redefining terms and words, allowing clarity to emerge where there was none before. However, in my view, it is equally important to recognize that redefining terms cannot serve merely as a justification for a prior premise. What is the benefit of inventing a telos for a claim if it does not reflect reality?
Therefore, I will redefine “environment” in a way that follows naturally from necessary existence and ontological distinction, while also incorporating the mainstream definition. It is not sufficient for a philosophical system to be internally coherent, valid, and sound. It must also align with empirically established consensus.

Now, in Nilogism:
The environment is a phenomenological consequence arising from the relation between a cognitive system and existence, establishing the boundary between self and non-self. More precisely, this consequence consists of everything that is non-self, and this is what I refer to as the environment.

To better illustrate what I mean by this, imagine a first-person video game. Everything within the player’s field of view is rendered and appears on the screen. However, this does not mean that what lies outside the field of view does not exist. The code for the entire game is already written and processed, but the graphics are rendered only when the player changes their perspective.
In this sense, existence surrounds the player entirely, yet the player can access only a portion of it, namely, what appears within their perspective. This accessible portion is what I define as the environment.

The environment cannot be ontological, because if we remove all forms of perception from existence, what remains? A rock is still a rock, even without a brain distinguishing it from the rest. Its existence is unaltered by the absence of perception.
However, when perception separates this rock from its surroundings, that particular rock becomes part of the environment of a cognitive system. Nothing changes about the rock itself, but something does change about the perceiver. The rock now exists not only ontologically, but also teleologically. By being perceived, it is assigned a function.

From this perspective, one might now ask: if the environment is our perception and evaluation of existence, what about family, friends, and acquaintances? If the environment is the non-self, how do multiple non-selves coexist and communicate?

This is the natural implication, and the next logical question.

Our parents, children, grandchildren, cousins, aunts, brothers, and sisters are all part of the non-self. A child is not their parent in precisely the same way that a parent is not their child. What occurs here, I would argue, is the operation of the principle of Projection.

If the principle of Distinction is the manifestation of existence, then the principle of Projection is the manifestation of the self upon the non-self.
Think of this as a zooming-in from necessary existence into the interior of a self.

It is redundant to question the existence of the self, because it exists. It is me, it is you, and it is every human being on this planet, every human being who has existed and who will exist.

Every human being is a self. A self is the entire ladder of consciousness combined with necessary existence at the bottom and free will at the top. The self is the localization of existence. Existence, as a fundamental necessity, is distinction and upon this distinction, ladders of consciousness evolve into free wills.
For this reason, the self cannot perceive existence as a whole. If existence were a single wave, it would be perceived all at once. But we are distinct units, each perceiving only a portion of existence. Because we are distinct, our perception is also distinct. Therefore, the environment is relational to each self.
And because the environment is a correlate of each individual self, we gain the ability to shape the environment according to our will, but not existence itself. This capacity to exert influence upon the environment is what I call the principle of Projection.

When a self perceives a part of existence, that portion becomes the self’s environment. As such, the self can act upon its environment. It can recognize another self, and this recognition extends the environment to include that other self, and vice versa.
Because another self becomes part of the environment, it can either be ignored or acted upon. When it is acted upon, this part of the environment, this other self, is no longer indifferent to us. This loss of indifference is the projection of the self. When the self projects itself onto the environment, it constitutes a form of internalization, or, in other words, a conceptual expansion of the within to encompass part of the non-self.
In this way, a larger and more complex distinction is formed: one rooted in the ontological boundary of the necessary within, and extended by an added conceptual boundary imposed upon parts of the environment.
This is why family members are often as important to us as ourselves, and sometimes even more so. We place them within the closest conceptual boundary, directly adjacent to our ontological boundary. They thus become a form of non-self whose absence would leave our ontological boundary exposed to an indifferent environment.

This is why I said that I would save my own child from the burning building, rather than a scientist who has the potential to save the entire human race.

Saving my child would neither be a choice nor an act of bravery. It would simply be the strongest deterministic impulse to protect my within from the indifferent environment. In that moment, I would not be exercising free will, I would be allowing my most primitive drive to take control of the self in order to protect what I have internalized as my own. In saving my child, I would, in effect, be saving myself.

By contrast, if a soldier were to risk their life to save a colleague from an entirely different unit, someone they may have interacted with only a few times, this would constitute genuine bravery. In such a case, they would be protecting a distant conceptual boundary, one located close to the indifferent environment. Here, the deterministic impulse toward self-preservation is halted, and action is redirected toward saving someone largely unrelated to the self. In doing so, they exercise free will.

And if, on their way to save that colleague, they were to notice another colleague, one more intimately connected to their ontological within, they would once again redirect their action from the more distant conceptual boundary to the closer one. They would allow impulse to take over, thereby exercising less free will.

Now, imagine this mechanism applied across all human beings. From this, we can understand why morality begins with the universal and fundamental bedrock of preserving human life. Morality is simply the intersection of multiple environments, giving rise to laws and rights that preserve society and enable the scaffolding of knowledge, developed over thousands of years, through the instinctual projection of selves.


## Chapter 11 - Last men or Resistance 

Since I finished writing Chapter 7, I began releasing the key concepts of Nilogism into the wild, into the deep philosophical jungles of mighty Reddit itself. At that time, I still lacked the precise lexicon needed to properly convey what I believed to be easily understood. 
To borrow Zarathustra’s words, *“my mouth was not for their ears”*, not because they could not understand, but because my "mouth" itself was not yet ready.

Nevertheless, about a month ago, I was fortunate to enter into an ongoing correspondence with a mathematician who resonated with the idea that free will is resistance to the most predictable outcome. This exchange opened my eyes to my own protected use of the term "environment", and from this realization, Chapter 10 came into existence.

Thus Chapter 10 marks the first time Nilogism stepped into the "desert of the real" and returned accompanied by a thinker, one whose disagreement rendered Nilogism more precise and more grounded in reality.

Moving on, a few days ago I received a rather emotional response to that same definition of free will, this time under a different post.

The response criticized my work on the grounds that I had contributed nothing novel to the philosophical debate. 
While I did not see a substantive counterargument, I did observe an attempt to dismantle the nilogist account of free will through semantic manipulation of my own vocabulary. I sensed the emotional charge when it was implied that, if the term “free will” were replaced with “will”, nothing would change, and that, therefore, I had failed at philosophy.

I will be honest: a certain anger took hold of me. Yet, despite the impulse to defend my work against what I perceived as an unfair attack, I chose instead to simply ask what had sparked such an emotional response to my words. In return, I was accused of being the emotional one in this exchange.

For those unfamiliar with Reddit, especially the debate-oriented subforums, it is a ruthless platform, though also one where some participants do engage in good faith, while others do not.

I am an amator (not amateur) of philosophical debate, and I do spend time within these spaces.

This encounter led me to realize something important. If, for this person, the word “free” did not sit comfortably next to “will”, then it became clear to me that I must first understand what the "will" itself is: how it functions, and why it matters. From there, I quickly arrived at the conclusion that the "will" is necessarily a part of a hierarchy.

Under this particular circumstance, I will not appeal to mainstream definitions found in dictionaries, as we have now left mainstream terrain. And, of course, when discussing the "will", we must turn to none other than the philosopher most closely associated with it.

Arthur Schopenhauer was a legendary German philosopher, best known for his 1818 work *"The World as Will and Representation"* (expanded in 1844), in which he characterizes the phenomenal world as the manifestation of a blind and irrational noumenal will.

He famously wrote:
*“A man can do what he wills, but he cannot will what he wills.”*

Standing on the scaffolding provided by this giant of philosophy, I came to reason that the "will" is not the container in which the process of choice unfolds, but rather a phenomenological manifestation arising from more primitive drives.
With this in mind, I can now clearly distinguish between **a reaction, a need, an impulse, and a want.**

It also becomes evident that the term "will" is the appropriate term for describing deterministic behavior.

From my perspective, a **reaction** and a **need** are, counterintuitively, two opposite terms.

A **reaction** is a response to external stimuli originating from the environment. If you recall my example of *“Mary feeling cold”* from Chapter 7, I argued that *“the feeling”* of cold is a reaction, defined as the cognitive process of turning input into preservation. As such, the input originates externally, from the environment. Ergo, it is the environment that drives us to protect ourselves from its harsh conditions, and we cannot resist this drive, as it is **essential** to immediate survival and persistence through time.

A **need**, by contrast, is a response to internal stimuli. Hunger, thirst, and even the drive for reproduction are likewise cognitive processes of turning input into preservation, but here, the input originates from **within**. These are "alarms" relayed by our organs to the brain in the form of hunger and thirst. A need, too, is **essential** to immediate survival and long-term persistence.

*At this point, it is worth noting that while the need for reproduction is present across all forms of life, albeit through different mechanisms, humans are not unique in decoupling sexual pleasure from reproduction. Some advanced animals also engage in sexual activity for pleasure or as a social behavior, not exclusively for reproductive purposes.*

**Reactions** and **needs** together form the first rung of the default behavioral hierarchy. They represent drives that threaten the very boundary of the life form if left unaddressed, which is precisely why Schopenhauer’s observation holds true.

However, the second rung is where complexity emerges, in the form of **inessentiality**. Here, we encounter the impulse.

Before defining **impulse**, we must first recognize that when we engage in the act of choosing, we are effectively calculating the allocation of **three resources**: **physical or intellectual effort, external material or conceptual currency,** and **time.**
By external material currency, I do not necessarily mean money, but rather anything materially valuable that one possesses and can exchange.

An **impulse** arises when the environment introduces a cause that nudges the self toward action. Unlike reactions and needs, an impulse is not essential to immediate survival, nor does ignoring it threaten the well-being of the self.

For example: *I may feel the urge to eat an entire cake because it looks extremely delicious, but I might settle for just a slice.*

The initial urge to eat the whole cake is an impulse. I can prevent it from being enacted precisely because it is **not imperative** to my survival. However, an impulse becomes increasingly difficult to resist when its **cost is low**.
If I am already sitting at the table where the cake is served, I may eat it simply because I do not need to expend additional **effort**, **time**, or **material resources**, such as going to the shop, spending money, or interrupting my current activity.

Under this same rung, beyond the reach of impulses, a cognitive process begins, one that is always in motion, like a "mathematical engine" that never turns off. 
I am confident in locating this engine at the intersection of the subconscious and conscious. 

Based on the allocation of the three resources, this engine operates more deeply within the subconscious as costs decrease, manifesting as impulses, and it crosses into the conscious domain as costs become more substantial.

On the conscious side, I would argue that the "will" begins to reveal itself, more precisely, in the space between competing possibilities of action.

Let us now analyze a simple statement:
*“I want to go out for a meal.”*

This is clearly a want, not yet a choice. It is a desire, but, in my view, not "will".

This desire first requires the prediction of possible paths that could be followed to achieve it.

Prediction, however, is only possible after introspective investigation into what caused the desire to arise. 

*To clarify, I do not believe that we always identify the causes of our actions. In fact, by default, we usually do not, as everyday life largely revolves around impulse management. In this example, however, 'going out for a meal' is not an impulse, and for such more complex actions, we often do identify their causes.*

I stated that the "will" becomes evident in the conscious realm because it requires the ability to identify both the cause of a desire and its predicted cost.

*“I need to get ready, dress up, and drive there.”*
Here, cognition has calculated the **predicted effort**, based on previous experiences stored in memory.

*“I am a bit tight on money, so if I go, I might not order what I really want.”*
Here, cognition has calculated the **predicted currency** cost of the endeavor.

*“I am working tomorrow, so I should not spend too much time out.”*
Here, cognition has calculated the cost of **time**.

At this point, all resources have been allocated, and the outcome feels tangible.

*“But I also feel tired after today.”*
Something different occurs here: **a new variable emerges**. This is the result of estimating the overall cost of fulfilling the initial desire. A new possibility appears, one **not caused** directly by the original desire, but by the visualization of resource allocation required to satisfy it. This new possibility represents zero cost.

At this moment, the collapse between two possible outcomes becomes imminent.

*“I haven’t been out for a very long time; I deserve a little treat.”*
This actuation, in my view, is the "will". It is the capacity to rise above impulse and maintain the initial vector despite the most obvious alternative paths. "Will" is the process that allows us to care for ourselves, to acknowledge our own hardships, and to act in accordance with our long-term self-benefit, by allocating resources toward consolidating the self, even when cheaper paths with less satisfaction are available.

*“It’s not worth it. If it were Friday, maybe.”*
In this case, the costs are judged to be too great to bear, and **additional conditions**, such as fatigue, lead to the conservation of resources rather than their expenditure.

Now, having wrestled with what the "will" is, I will recapitulate what is "will" and "free will" by conjuring a hypothetical scenario:

The Nilogist Afternoon:

​**Reaction** *(Environment → Self)*: You touch a hot mug. Your hand jerks back. You didn't "think"—the environment forced a change in your state.

**​Need** *(Within → Brain)*: An hour later, your stomach growls. The "internal alarm" goes off. You must eat to maintain your boundary.

**​Impulse** *(Low-cost Nudge)*: You see a bowl of candy. It’s right there *(zero effort, zero currency)*. You reach for it without a second thought.

​**Want** *(Desire + Calculation)*: You think, "I want a real dinner." The "Mathematical Engine" starts. You calculate the Effort (cooking), Currency (buying groceries), and Time.

**​Choice** *(Wavefront Collapse)*: Based on your resources, the engine settles on "Pasta." This is the most efficient output of your current state.

**​Will** *(Executing the Vector)*: You are tired, but you stand up and cook anyway. You are "paying" the effort to achieve the higher satisfaction. You are following the "Best Deal."

**​Free Will** *(The Override)*: Just as you sit to eat, a neighbor knocks, distressed and needing help. The "obvious" path is to eat your hot meal. But you resist that deterministic impulse and set the plate aside to help them. That is the only moment in the whole afternoon where you were "Free.”

It now appears that a binary system becomes visible through the interplay between will and free will.

With this distinction in place, we can better understand what the “free” in free will actually signifies.

Will is the deterministic capacity to allocate conceptual and material resources toward the stability and cultivation of the self.

It is also the necessary precondition for free will, which consists in the allocation of those same resources away from the self, without immediate self-benefit, yet teleologically justified.

Thus, the “free” in free will describes a freedom constrained by the distinction of the self (its genetics and environmental conditioning) through which second-order variables are able to alter the initial deterministic vector.

More precisely, we are free from being selfish, but only within the degrees allowed by our own existential necessity.

Or stated more explicitly, we are free to act in accordance with our values, whether those values are self-beneficial or not.

Through this mechanistic juggling, it is clear that heroism cannot simply be willed into existence. One does not suddenly decide to be heroic. Rather, one must first freely undertake a path of defiance against egotistical values, and this cannot be done in isolation or without knowledge.

Likewise, it is not necessarily easier to preserve oneself more if one’s upbringing has indoctrinated a deep sense of servitude. Yet, with sufficient knowledge and within a community, we do have the ability to correct this faulty initial vector imposed upon us.

This is not merely a matter of ambition, but of knowing oneself, for only through self-knowledge can ambition arise, either to oppose or to achieve something.

*“I have been raised this way”* is not an excuse to halt further progression, but the very spark of it. It is the seed that enables the eventual transcendence of having been “raised this way.”

*“I cannot help it,”* as expressed in addiction, reveals an imbalance of the will, one that begins at the rung of memory and propagates upward, impairing all higher rungs. The self becomes locked into allocating all available resources toward the most predictable outcome: the repetition of the initial cause that corrupted the memory rung in the first place.

For example:

**Memory (The Corruption):**
The memory of “relief” or “reward” becomes so dominant that it overrides all other reference points for cognitive evaluation.

**Language and Thought (The Rationalization):**
The mind begins to redefine “need” to include the addiction. Linguistic loops emerge: “I need this,” “This is the last time,” and, after repeated failures, “I need this to feel normal.”

**Choice (The Illusion of Wavefront Collapse):**
The “mathematical engine” is rigged. When possibilities are evaluated, the addicted outcome is so heavily weighted that the collapse occurs almost instantaneously. The “wave” does not meaningfully spread anymore. 

**Sentience and Consciousness (The Default Allocation):**
Because the Will (the engine) is compromised (which ordinarily optimizes for self-preservation), it is deceived into identifying the addiction as preservation itself. Effort, time, and currency are allocated toward the most predictable outcome, which is to reiterate the cause which lead to the corruption of the memory rung in the first place. 

Thus, the phrase *“I cannot help it”* is not a denial of agency, but a confession: the initial deterministic vector has become so dominant that second-order variables can no longer generate sufficient friction to redirect it.

Therefore addiction is not a "weak will", rather it is a Deceived Will.

I think it is only fair that I disclose the source from which I derived the existence of the three resources and their constant dance between allocation and conservation.

That source is the legendary philosopher who stood openly against rigid philosophical systems:
Friedrich Wilhelm Nietzsche.

Nietzsche began his career as a classical philologist before turning to philosophy early in his academic life, and it is precisely this sensitivity to language, movement, and embodiment that makes his insights into the will so sharp.

I will now present the quote that, upon first reading, instantaneously connected several unresolved threads within my thinking:

*“So let us for once be more cautious, let us be ‘unphilosophical’:
let us say that in all willing there is firstly a plurality of sensations, namely, the sensation of the condition ‘away from which we go’, the sensation of the condition ‘towards which we go’, the sensation of this ‘from’ and ‘towards’ itself, and then besides, an accompanying muscular sensation, which, even without our putting in motion ‘arms and legs,’ commences its action by force of habit, directly we ‘will’ anything.”*

This passage appears in *“Beyond Good and Evil”*, aphorism 19.

Let me now offer my **own** interpretation:

Nietzsche begins by reminding us that the self is not a singular force, but a plurality of sensations, feelings, emotions, tensions, anticipations. From this plurality, he isolates those sensations most relevant to action.

First, he identifies the *“sensation of the condition away from which we go.”* 
Here, Nietzsche points to our capacity to recognize a causal pressure acting upon us, to become aware that something is pushing, repelling, or destabilizing us. We feel the source of the nudge.

He then identifies the *“sensation of the condition towards which we go.”* Once the causal vector is recognized, we are able to anticipate the possible paths it opens. We project forward, imagining where this force may carry us.

Next, Nietzsche highlights the *“sensation of this ‘from’ and ‘towards’ itself.”* This is crucial. It refers to our awareness of the relation between cause and effect as it pertains to the self and what it would mean for us to traverse that space.

Finally, he introduces what I consider the decisive insight:
the *“accompanying muscular sensation”* that arises before any physical motion occurs. Even without moving “arms or legs”, the body begins to simulate action through habit.
This reveals that in the space between cause and predicted effect, we internally simulate the effort required to move from one to the other, before we choose.

From this Nietzschean insight, several clarifications followed naturally:

First, I realized the necessity of sharply distinguishing reaction and need. Regardless of effort, both must be satisfied under all circumstances, for they are imperative to survival itself.

Second, I was finally able to properly define what I had previously referred to (somewhat loosely) as an impulse. An impulse is a low-effort potential action that can be inhibited precisely because it is not essential to the maintenance of the self.

And from there, it became evident that effort alone is insufficient as a metric. Time and material resources must also be expended, conserved, or risked.

Consider something as mundane as waking up at five in the morning. If you lack the discipline to rise immediately when the alarm sounds (as many of us do, myself included) you may find yourself internally shouting at yourself to get out of bed.
That inner struggle is not poetic metaphor. It is the will calculating effort, time, and cost. 
